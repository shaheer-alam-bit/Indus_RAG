{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11419681,"sourceType":"datasetVersion","datasetId":7151941},{"sourceId":11436976,"sourceType":"datasetVersion","datasetId":7163922}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install langchain_community\n# !pip install faiss-gpu\n# !pip install rank_bm25 \n# !pip install ragas\n# !pip install datasets\n# !pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:12.887266Z","iopub.execute_input":"2025-04-17T14:48:12.887545Z","iopub.status.idle":"2025-04-17T14:48:12.891320Z","shell.execute_reply.started":"2025-04-17T14:48:12.887521Z","shell.execute_reply":"2025-04-17T14:48:12.890278Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:12.892607Z","iopub.execute_input":"2025-04-17T14:48:12.892915Z","iopub.status.idle":"2025-04-17T14:48:12.904673Z","shell.execute_reply.started":"2025-04-17T14:48:12.892893Z","shell.execute_reply":"2025-04-17T14:48:12.903712Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pdfplumber\n\n# Open the PDF file\nwith pdfplumber.open('/kaggle/input/star-wars/Star Wars - Brotherhood Mike Chen.pdf') as pdf:\n    # Open the text file for writing\n    with open('knowledge_base.txt', 'w', encoding='utf-8') as output:\n        # Iterate over pages 10 to 349 (0-indexed, so subtract 1)\n        for i in range(9, 349):  # Page 10 is index 9\n            page = pdf.pages[i]\n            text = page.extract_text() or \"\"  # Handle cases where text is None\n            output.write(text + '\\n')  # Write text to file with a newline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:12.905692Z","iopub.execute_input":"2025-04-17T14:48:12.906020Z","iopub.status.idle":"2025-04-17T14:48:41.283122Z","shell.execute_reply.started":"2025-04-17T14:48:12.905989Z","shell.execute_reply":"2025-04-17T14:48:41.282434Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import pipeline\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport warnings\n\nfrom datasets import Dataset\nwarnings.filterwarnings(\"ignore\")\nimport textwrap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:41.283903Z","iopub.execute_input":"2025-04-17T14:48:41.284217Z","iopub.status.idle":"2025-04-17T14:48:48.995436Z","shell.execute_reply.started":"2025-04-17T14:48:41.284196Z","shell.execute_reply":"2025-04-17T14:48:48.994822Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 1. Load your text files\nfile_paths = [\"/kaggle/working/knowledge_base.txt\"]\ndocuments = []\n\nfor file_path in file_paths:\n    loader = TextLoader(file_path)\n    docs = loader.load()\n    documents.extend(docs)\n\n# 2. Define chunking parameters\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,        # You can try 100, 250, 512, etc.\n    chunk_overlap=512       # Try 0, 50, 100, etc.\n)\n\n# 3. Split the documents\nchunks = text_splitter.split_documents(documents)\n\n\n# 4. Output result\nprint(f\"Total chunks: {len(chunks)}\")\nprint(f\"First chunk content:\\n{chunks[0].page_content}\")\n\n# Optional: Save the chunks to a file\nwith open(\"chunked_output.txt\", \"w\", encoding='utf-8') as f:\n    for i, chunk in enumerate(chunks):\n        f.write(f\"--- Chunk {i + 1} ---\\n\")\n        f.write(chunk.page_content + \"\\n\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:48.997665Z","iopub.execute_input":"2025-04-17T14:48:48.998278Z","iopub.status.idle":"2025-04-17T14:48:49.035100Z","shell.execute_reply.started":"2025-04-17T14:48:48.998252Z","shell.execute_reply":"2025-04-17T14:48:49.034282Z"}},"outputs":[{"name":"stdout","text":"Total chunks: 1198\nFirst chunk content:\nA long time ago in a galaxy far, far away….\nThe CLONE WARS have erupted. Caught off guard by the quickly\nexpanding conflict, the overwhelmed Jedi Order has rushed the\nadvancement of Padawans to better integrate into the Grand Army of the\nRepublic and assist the war effort.\nNewly promoted Jedi Knight Anakin Skywalker is increasingly torn\nbetween his growing duties to the Republic and his secret marriage to\nSenator Padmé Amidala of Naboo. With his Knighting, his mentor Obi-\nWan Kenobi has been elevated to the Jedi Council under the rank of Jedi\nMaster.\nAs dark forces push the Jedi further toward their transformation from\nguardians to soldiers, Anakin and Obi-Wan find themselves on equal\nfooting yet opposing paths, each pondering the meaning of peace and\njustice during a time of war…\nCHAPTER 1\nRUUG QUARNOM\nCato Neimoidia was a world of mist.\nHigh above that mist, cliffs and branches poked through, carved at all\nangles into immense mountainous spires. The thick stone of the planet’s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Prepare documents and their metadata\ntexts = [chunk.page_content for chunk in chunks]\nmetadata = [chunk.metadata for chunk in chunks]\nprint(len(texts))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:49.036569Z","iopub.execute_input":"2025-04-17T14:48:49.036887Z","iopub.status.idle":"2025-04-17T14:48:49.041498Z","shell.execute_reply.started":"2025-04-17T14:48:49.036857Z","shell.execute_reply":"2025-04-17T14:48:49.040563Z"}},"outputs":[{"name":"stdout","text":"1198\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Initialize embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n\n# Create FAISS vector database\nvectordb = FAISS.from_documents(chunks, embedding_model)\n\n# Save FAISS index to disk for later use\nvectordb.save_local(\"faiss_index\")\n\n# Check the number of stored documents\nprint(f\"Number of documents in the vector store: {vectordb.index.ntotal}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:49.042345Z","iopub.execute_input":"2025-04-17T14:48:49.042565Z","iopub.status.idle":"2025-04-17T14:48:56.821506Z","shell.execute_reply.started":"2025-04-17T14:48:49.042545Z","shell.execute_reply":"2025-04-17T14:48:56.820697Z"}},"outputs":[{"name":"stdout","text":"Number of documents in the vector store: 1198\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# BM25 Indexing\ntokenized_texts = [text.split() for text in texts]\nbm25 = BM25Okapi(tokenized_texts)\n\ndef reciprocal_rank_fusion(results_bm25, results_embedding, k=2):\n    scores = {}\n\n    # Use document content or metadata as the key\n    for rank, (doc, score) in enumerate(results_bm25):\n        doc_id = doc.page_content  # Or use doc.metadata.get(\"source\", \"unknown\") if available\n        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n        print(\"BM25\", scores[doc_id])\n\n    for rank, (doc, score) in enumerate(results_embedding):\n        doc_id = doc.page_content  # Use the same identifier\n        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n        print(\"Dense\", scores[doc_id])\n\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n\n# Extract page content and metadata properly\ndef format_response(doc):\n    return f\"Page {doc.metadata.get('page', 'Unknown')}: {doc.page_content.strip()}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.822338Z","iopub.execute_input":"2025-04-17T14:48:56.822589Z","iopub.status.idle":"2025-04-17T14:48:56.912516Z","shell.execute_reply.started":"2025-04-17T14:48:56.822568Z","shell.execute_reply":"2025-04-17T14:48:56.911604Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Retrieve function\ndef retrieve(query, k=3):\n    query_embedding = embedding_model.embed_query(query)\n    results_embedding = vectordb.similarity_search_with_score_by_vector(query_embedding, k=k)\n    results_embedding = sorted(results_embedding, key=lambda x: x[1], reverse=True)\n    \n    print(\"============Dense Embeddings=============\")\n    for doc, score in results_embedding:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n\n    # Get BM25 scores for all documents and sort to get top-k results\n    results_bm25 = [(idx, bm25.get_scores(query.split())[idx]) for idx in range(len(texts))]\n    results_bm25 = sorted(results_bm25, key=lambda x: x[1], reverse=True)[:k]  # Keep only top-k results\n    # Convert BM25 results to (Document, score) format\n    results_bm25_docs = [(Document(page_content=texts[idx], metadata=metadata[idx]), score) for idx, score in results_bm25]\n   \n    print(\"************BM25 Results*************\")\n    for doc, score in results_bm25_docs:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n    \n    # Create a lookup dictionary {document content -> Document object}\n    doc_lookup = {doc.page_content: doc for doc, _ in results_bm25_docs}\n    doc_lookup.update({doc.page_content: doc for doc, _ in results_embedding})\n\n    # Fuse results\n    fused_results = reciprocal_rank_fusion(results_bm25_docs, results_embedding)\n    \n    # Format results, ensuring document IDs are mapped back to actual Documents\n    return [format_response(doc_lookup[doc_id]) for doc_id, _ in fused_results if doc_id in doc_lookup]\n\n    #fused_results = reciprocal_rank_fusion(results_bm25, results_embedding)\n    #return [(texts[idx], metadata[idx][\"page\"] if \"page\" in metadata[idx] else \"Unknown\") for idx, _ in fused_results]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.913474Z","iopub.execute_input":"2025-04-17T14:48:56.913816Z","iopub.status.idle":"2025-04-17T14:48:56.920338Z","shell.execute_reply.started":"2025-04-17T14:48:56.913782Z","shell.execute_reply":"2025-04-17T14:48:56.919497Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from langchain.schema import Document\n\ndef retrieve_dense(query, k=3):\n    query_embedding = embedding_model.embed_query(query)\n    results_embedding = vectordb.similarity_search_with_score_by_vector(query_embedding, k=k)\n    \n    # Optionally sort descending by score if needed\n    results_embedding = sorted(results_embedding, key=lambda x: x[1], reverse=True)\n    \n    print(\"============Dense Embeddings=============\")\n    for doc, score in results_embedding:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n    \n    # Return just the documents (or both doc and score if you want)\n    return [format_response(doc) for doc, _ in results_embedding]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.921055Z","iopub.execute_input":"2025-04-17T14:48:56.921267Z","iopub.status.idle":"2025-04-17T14:48:56.934723Z","shell.execute_reply.started":"2025-04-17T14:48:56.921248Z","shell.execute_reply":"2025-04-17T14:48:56.934036Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Query example\nquestion = \"What is the name of the Neimoidian guard who assists Obi-Wan?\"\nretrieved_responses = retrieve_dense(question, k=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.935543Z","iopub.execute_input":"2025-04-17T14:48:56.935838Z","iopub.status.idle":"2025-04-17T14:48:56.968696Z","shell.execute_reply.started":"2025-04-17T14:48:56.935809Z","shell.execute_reply":"2025-04-17T14:48:56.967901Z"}},"outputs":[{"name":"stdout","text":"============Dense Embeddings=============\npage Unknown - Score: 0.2202 - formation. Obi-Wan put his hands up, then glanced upward at the tower, a\nquick flick of the Force to...\npage Unknown - Score: 0.2200 - “Please, hurry, this guard needs medical attention!” Obi-Wan’s voice was\nnearly as loud as Ketar’s. ...\npage Unknown - Score: 0.2199 - honest with me.”\nA low hum vibrated from the shackles, the frequency of their energy\nsubtle enough t...\npage Unknown - Score: 0.2180 - guard platforms.\n“I appreciate what you did in the hallway,” Obi-Wan said without\nresistance.\nRuug h...\npage Unknown - Score: 0.1876 - tone.\n“I’m staying here. I’m an active guard. But Ketar will identify me as a\ntraitor. My best chanc...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# # Query processing\n# question = \"What was the cause of the bombing on Cato Neimoidia in Brotherhood?\"\n# retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n# docs = retriever.get_relevant_documents(question)\n\n# # Print results\n# for i, doc in enumerate(docs, 1):\n#     page_number = doc.metadata.get('page', 'Unknown')\n#     # print(f\"Document {i} - Page {page_number} - Score: {doc.metadata.get('score', 'N/A')}\")\n#     print(doc.page_content[:])  # Print first 500 characters of each result\n#     print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.969523Z","iopub.execute_input":"2025-04-17T14:48:56.969706Z","iopub.status.idle":"2025-04-17T14:48:56.972869Z","shell.execute_reply.started":"2025-04-17T14:48:56.969689Z","shell.execute_reply":"2025-04-17T14:48:56.972052Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# model_name = \"tiiuae/Falcon3-3B-Instruct\"\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\", #device_map='cuda'\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:48:56.973586Z","iopub.execute_input":"2025-04-17T14:48:56.973827Z","iopub.status.idle":"2025-04-17T14:49:04.236544Z","shell.execute_reply.started":"2025-04-17T14:48:56.973807Z","shell.execute_reply":"2025-04-17T14:49:04.235823Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9b307f2ef7455c999b5f0b192e93e5"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Create a pipeline\ngenerator = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nreturn_full_text=False,\nmax_new_tokens=5000,\ndo_sample=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:49:04.237250Z","iopub.execute_input":"2025-04-17T14:49:04.237495Z","iopub.status.idle":"2025-04-17T14:49:04.244224Z","shell.execute_reply.started":"2025-04-17T14:49:04.237474Z","shell.execute_reply":"2025-04-17T14:49:04.243265Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# # Extract page content and metadata properly\n# def format_response(doc):\n#     return f\"Page {doc.metadata.get('page', 'Unknown')}: {doc.page_content.strip()}\"\n\n# # Handle cases where fewer than 3 results are returned\n# retrieved_responses = [format_response(doc) for doc in docs[:3]]\n# while len(retrieved_responses) < 3:\n#     retrieved_responses.append(\"No relevant information.\")  # Fill missing slots","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:49:04.245123Z","iopub.execute_input":"2025-04-17T14:49:04.245412Z","iopub.status.idle":"2025-04-17T14:49:04.256762Z","shell.execute_reply.started":"2025-04-17T14:49:04.245389Z","shell.execute_reply":"2025-04-17T14:49:04.255709Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def summarize_responses(responses, max_summary_length=1000):\n    from transformers import pipeline\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n    concatenated_responses = \" \".join(responses)\n    summarized = summarizer(concatenated_responses, max_length=max_summary_length, min_length=50, do_sample=False)\n    return summarized[0]['summary_text']\n\n# Generate summarized responses\nsummarized_responses = summarize_responses(retrieved_responses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:51:59.999184Z","iopub.execute_input":"2025-04-17T14:51:59.999573Z","iopub.status.idle":"2025-04-17T14:52:01.938299Z","shell.execute_reply.started":"2025-04-17T14:51:59.999539Z","shell.execute_reply":"2025-04-17T14:52:01.936958Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-a544708d9101>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Generate summarized responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msummarized_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_responses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-a544708d9101>\u001b[0m in \u001b[0;36msummarize_responses\u001b[0;34m(responses, max_summary_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_summary_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconcatenated_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msummarized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_responses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_summary_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         self.check_model_type(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         ):\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;31m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                 )\n\u001b[0;32m-> 3164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# ### **Retrieved Information**:\n# 1. {retrieved_responses[0]}\n# 2. {retrieved_responses[1]}\n# 3. {retrieved_responses[2]}\n# 4. {retrieved_responses[3]}\n# 5. {retrieved_responses[4]}\n\n# Construct the RAG prompt\nprompt = f\"\"\"\nYou are an AI assistant tasked with answering questions based on retrieved knowledge.\n\n### **Summarized Retrieved Information**:\n{summarized_responses}\n\n### **Question**:\n{question}\n\n### **Instructions**:\n- Integrate the key points from all retrieved responses into a **cohesive, well-structured answer**.\n- If the responses are **contradictory**, mention the different perspectives.\n- If none of the retrieved responses contain relevant information, reply:\n  **\"I couldn't find a good response to your query in the database.\"**\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:49:28.803496Z","iopub.status.idle":"2025-04-17T14:49:28.803928Z","shell.execute_reply":"2025-04-17T14:49:28.803723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0,len(retrieved_responses)):\n    print(retrieved_responses[i])\n    print(\"-------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:49:28.804584Z","iopub.status.idle":"2025-04-17T14:49:28.805000Z","shell.execute_reply":"2025-04-17T14:49:28.804820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use Qwen2.5 3B with the correct message format\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\n# Generate output using the model\noutput = generator(messages)\n\n# Print formatted response\nprint(textwrap.fill(output[0][\"generated_text\"], width=80))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:49:28.805616Z","iopub.status.idle":"2025-04-17T14:49:28.806990Z","shell.execute_reply":"2025-04-17T14:49:28.806871Z"}},"outputs":[],"execution_count":null}]}