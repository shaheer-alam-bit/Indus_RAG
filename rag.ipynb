{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11419681,"sourceType":"datasetVersion","datasetId":7151941},{"sourceId":11436976,"sourceType":"datasetVersion","datasetId":7163922}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain_community\n!pip install faiss-gpu\n!pip install rank_bm25 \n!pip install ragas\n!pip install datasets\n!pip install pdfplumber\n!pip install azure-openai\n!pip install sumy","metadata":{"_uuid":"68888433-599e-47af-87f9-757373d26409","_cell_guid":"43b6e81c-ee35-4ed7-ae06-713326188627","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.getcwd()\n\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import pipeline\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport warnings\n\nfrom datasets import Dataset\nwarnings.filterwarnings(\"ignore\")\nimport textwrap","metadata":{"_uuid":"df2e31d9-345e-4152-ac3e-5118926cf9dd","_cell_guid":"b9c8878b-d3a8-499e-a7ea-cf49f28c0c7a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pdfplumber\n\n# Open the PDF file\nwith pdfplumber.open('/kaggle/input/star-wars/Star Wars - Brotherhood Mike Chen.pdf') as pdf:\n    # Open the text file for writing\n    with open('knowledge_base.txt', 'w', encoding='utf-8') as output:\n        # Iterate over pages 10 to 349 (0-indexed, so subtract 1)\n        for i in range(9, 349):  # Page 10 is index 9\n            page = pdf.pages[i]\n            text = page.extract_text() or \"\"  # Handle cases where text is None\n            output.write(text + '\\n')  # Write text to file with a newline","metadata":{"_uuid":"4246a54e-639f-4dec-9177-350de1f09f7b","_cell_guid":"9dbfc945-e02c-4924-98b6-6deb22373da0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load your text files\nfile_paths = [\"/kaggle/working/knowledge_base.txt\"]\ndocuments = []\n\nfor file_path in file_paths:\n    loader = TextLoader(file_path)\n    docs = loader.load()\n    documents.extend(docs)\n\n# 2. Define chunking parameters\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1200,        # You can try 100, 250, 512, etc.\n    chunk_overlap=600       # Try 0, 50, 100, etc.\n)\n\n# 3. Split the documents\nchunks = text_splitter.split_documents(documents)\n\n\n# 4. Output result\nprint(f\"Total chunks: {len(chunks)}\")\nprint(f\"First chunk content:\\n{chunks[0].page_content}\")\n\n# Optional: Save the chunks to a file\nwith open(\"chunked_output.txt\", \"w\", encoding='utf-8') as f:\n    for i, chunk in enumerate(chunks):\n        f.write(f\"--- Chunk {i + 1} ---\\n\")\n        f.write(chunk.page_content + \"\\n\\n\")","metadata":{"_uuid":"6778484e-9b56-40fd-8967-d8eec65ea1d2","_cell_guid":"3e46c168-6f57-41ee-9d80-9a6f06222a66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare documents and their metadata\ntexts = [chunk.page_content for chunk in chunks]\nmetadata = [chunk.metadata for chunk in chunks]\nprint(len(texts))","metadata":{"_uuid":"af625c44-ae0b-49f0-b5fa-8a5e893e1fd7","_cell_guid":"5952ff92-3356-46bb-9232-6e3d4fa1c397","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n\n# Create FAISS vector database\nvectordb = FAISS.from_documents(chunks, embedding_model)\n\n# Save FAISS index to disk for later use\nvectordb.save_local(\"faiss_index\")\n\n# Check the number of stored documents\nprint(f\"Number of documents in the vector store: {vectordb.index.ntotal}\")","metadata":{"_uuid":"4100184e-ffdf-42a7-9a2f-3424d9d62cca","_cell_guid":"bd5d7fe5-ef1f-476b-b7c1-6889d2109fa9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BM25 Indexing\ntokenized_texts = [text.split() for text in texts]\nbm25 = BM25Okapi(tokenized_texts)\n\ndef reciprocal_rank_fusion(results_bm25, results_embedding, k=2):\n    scores = {}\n\n    # Use document content or metadata as the key\n    for rank, (doc, score) in enumerate(results_bm25):\n        doc_id = doc.page_content  # Or use doc.metadata.get(\"source\", \"unknown\") if available\n        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n        print(\"BM25\", scores[doc_id])\n\n    for rank, (doc, score) in enumerate(results_embedding):\n        doc_id = doc.page_content  # Use the same identifier\n        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n        print(\"Dense\", scores[doc_id])\n\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n\n# Extract page content and metadata properly\ndef format_response(doc):\n    return f\"Page {doc.metadata.get('page', 'Unknown')}: {doc.page_content.strip()}\"","metadata":{"_uuid":"cd1062d0-2dfe-4a6d-b452-0822dacc9463","_cell_guid":"26320b8d-fee4-4b59-8f00-06ac4a34c733","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve function\ndef retrieve(query, k=3):\n    query_embedding = embedding_model.embed_query(query)\n    results_embedding = vectordb.similarity_search_with_score_by_vector(query_embedding, k=k)\n    results_embedding = sorted(results_embedding, key=lambda x: x[1], reverse=True)\n    \n    print(\"============Dense Embeddings=============\")\n    for doc, score in results_embedding:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n\n    # Get BM25 scores for all documents and sort to get top-k results\n    results_bm25 = [(idx, bm25.get_scores(query.split())[idx]) for idx in range(len(texts))]\n    results_bm25 = sorted(results_bm25, key=lambda x: x[1], reverse=True)[:k]  # Keep only top-k results\n    # Convert BM25 results to (Document, score) format\n    results_bm25_docs = [(Document(page_content=texts[idx], metadata=metadata[idx]), score) for idx, score in results_bm25]\n   \n    print(\"************BM25 Results*************\")\n    for doc, score in results_bm25_docs:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n    \n    # Create a lookup dictionary {document content -> Document object}\n    doc_lookup = {doc.page_content: doc for doc, _ in results_bm25_docs}\n    doc_lookup.update({doc.page_content: doc for doc, _ in results_embedding})\n\n    # Fuse results\n    fused_results = reciprocal_rank_fusion(results_bm25_docs, results_embedding)\n    \n    # Format results, ensuring document IDs are mapped back to actual Documents\n    return [format_response(doc_lookup[doc_id]) for doc_id, _ in fused_results if doc_id in doc_lookup]\n\n    #fused_results = reciprocal_rank_fusion(results_bm25, results_embedding)\n    #return [(texts[idx], metadata[idx][\"page\"] if \"page\" in metadata[idx] else \"Unknown\") for idx, _ in fused_results]","metadata":{"_uuid":"b246f6ab-cf66-46f5-a1d5-fce38f6e18b3","_cell_guid":"ffb44447-3b7d-4310-b600-de5a99812398","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.schema import Document\n\ndef retrieve_dense(query, k=3):\n    query_embedding = embedding_model.embed_query(query)\n    results_embedding = vectordb.similarity_search_with_score_by_vector(query_embedding, k=k)\n    \n    # Optionally sort descending by score if needed\n    results_embedding = sorted(results_embedding, key=lambda x: x[1], reverse=True)\n    \n    print(\"============Dense Embeddings=============\")\n    for doc, score in results_embedding:\n        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n    \n    # Return just the documents (or both doc and score if you want)\n    return [format_response(doc) for doc, _ in results_embedding]","metadata":{"_uuid":"63a91def-3040-47a8-9bdc-1172bea5908a","_cell_guid":"3550ba62-6e85-4c03-bd1c-e8219ba11566","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_BM25(query, k=3):\n    # Get BM25 scores for all documents\n    results_bm25 = [(idx, bm25.get_scores(query.split())[idx]) for idx in range(len(texts))]\n    \n    # Sort by score in descending order and select top-k\n    results_bm25 = sorted(results_bm25, key=lambda x: x[1], reverse=True)[:k]\n    \n    # Convert BM25 results to (Document, score) format\n    results_bm25_docs = [\n        (Document(page_content=texts[idx], metadata=metadata[idx]), score)\n        for idx, score in results_bm25\n    ]\n    \n    # Print results\n    print(\"*BM25 Results\")\n    for doc, score in results_bm25_docs:\n        print(f\"page {doc.metadata.get('page', 'Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n    \n    # Return only the Document objects\n    return [doc for doc, _ in results_bm25_docs]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_name = \"tiiuae/Falcon3-3B-Instruct\"\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\", #device_map='cuda'\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nstart_time = time.time()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Query example\nquestion = \"What is the name of the Neimoidian guard who assists Obi-Wan?\"\nretrieved_responses = retrieve_dense(question, k=6)","metadata":{"_uuid":"38b046b2-6005-4a92-bec8-2021c337d908","_cell_guid":"1b70c557-41d8-4cd4-a333-ffc43ca79a26","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Query processing\n# question = \"What was the cause of the bombing on Cato Neimoidia in Brotherhood?\"\n# retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n# docs = retriever.get_relevant_documents(question)\n\n# # Print results\n# for i, doc in enumerate(docs, 1):\n#     page_number = doc.metadata.get('page', 'Unknown')\n#     # print(f\"Document {i} - Page {page_number} - Score: {doc.metadata.get('score', 'N/A')}\")\n#     print(doc.page_content[:])  # Print first 500 characters of each result\n#     print(\"-\" * 80)","metadata":{"_uuid":"90c17a63-baee-4377-999f-c9e67e757b8b","_cell_guid":"0b133bdd-5159-4c97-926e-6052d40a66be","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a pipeline\ngenerator = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nreturn_full_text=False,\nmax_new_tokens=5000,\ndo_sample=False\n)","metadata":{"_uuid":"e1f46040-0335-4ea2-8e7f-7e341883074a","_cell_guid":"abed833f-9cf3-40e3-9266-44e8c25e4750","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sumy.parsers.plaintext import PlaintextParser\n# from sumy.nlp.tokenizers import Tokenizer\n# from sumy.summarizers.lsa import LsaSummarizer\n# import textwrap\n\n# def generate_lsa_summary(retrieved_responses, num_summary_sentence=50):\n#     # Combine the retrieved responses into one string\n#     text = \" \".join(retrieved_responses)\n    \n#     # Initialize LSA summarizer\n#     LANGUAGE = \"english\"\n#     parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n#     lsa_summarizer = LsaSummarizer()\n    \n#     # Generate the summary\n#     summary = []\n#     for sentence in lsa_summarizer(parser.document, num_summary_sentence):\n#         summary.append(str(sentence))\n    \n#     # Join the summarized sentences and wrap them for better readability\n#     summarized_text = \" \".join(summary)\n#     return textwrap.fill(summarized_text, 100)\n\n# summarized_responses = generate_lsa_summary(retrieved_responses)","metadata":{"_uuid":"d0f9c791-7b6c-4476-9840-8995d9479cd6","_cell_guid":"fe1f8113-5b96-41de-9d30-6988762c2b16","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reorder_sorted_responses(sorted_responses):\n    # Alternate between most important (edges) and least important (center)\n    most_important = sorted_responses[::2]  # Take every other response starting with the first\n    least_important = sorted_responses[1::2]  # Take every other response starting with the second\n\n    # Merge: Place least important in the center\n    reordered_responses = []\n    while most_important or least_important:\n        if most_important:\n            reordered_responses.append(most_important.pop(0))  # Add from most important\n        if least_important:\n            reordered_responses.append(least_important.pop())  # Add from least important\n    \n    return reordered_responses\nreordered_responses = reorder_sorted_responses(retrieved_responses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ### **Summarized Retrieved Information**:\n# {summarized_responses}\n\n# Construct the RAG prompt\nprompt = f\"\"\"\nYou are an AI assistant tasked with answering questions based on retrieved knowledge from the book Star Wars Brotherhood.\n\n### **Retrieved Information**:\n1. {reordered_responses[0]}\n2. {reordered_responses[1]}\n3. {reordered_responses[2]}\n4. {reordered_responses[3]}\n5. {reordered_responses[4]}\n6. {reordered_responses[5]}\n\n### **Question**:\n{question}\n\n### **Instructions**:\n- Integrate the key points from all retrieved responses into a **cohesive, well-structured answer**.\n- If the responses are **contradictory**, mention the different perspectives.\n- If none of the retrieved responses contain relevant information, reply:\n  **\"I couldn't find a good response to your query in the database.\"**\n\"\"\"","metadata":{"_uuid":"7c42a74a-9ad9-4ec8-97fa-a708b75a9ef1","_cell_guid":"7e8189cb-6a25-4382-abe2-dcd8cc775424","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0,len(reordered_responses)):\n    print(reordered_responses[i])\n    print(\"-------\")","metadata":{"_uuid":"92ae01f0-4b95-4661-bcdb-81836eb470cd","_cell_guid":"7378cecd-7fe4-4549-b482-719440822f1b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use Qwen2.5 3B with the correct message format\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\n# Generate output using the model\noutput = generator(messages)\n\n# Print formatted response\nprint(textwrap.fill(output[0][\"generated_text\"], width=80))","metadata":{"_uuid":"def30c04-baf3-44ee-9960-1c28fff493ae","_cell_guid":"592eda98-f21c-4c0d-89d7-8c1775908903","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"end_time = time.time()\ntime_taken = end_time - start_time\nprint(time_taken)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation Using RAGAS**","metadata":{}},{"cell_type":"code","source":"# from langchain_openai.chat_models import AzureChatOpenAI\n# from langchain_openai.embeddings import AzureOpenAIEmbeddings\n# from ragas.llms import LangchainLLMWrapper\n# from ragas.embeddings import LangchainEmbeddingsWrapper\n\n# azure_configs = {\n#     \"base_url\": \"https://sala-m9pmmei0-eastus2.cognitiveservices.azure.com/\",\n#     \"model_deployment\": \"my-gpt-deployment\",\n#     \"model_name\": \"gpt-4o-mini\",\n# }\n\n\n# azure_llm = AzureChatOpenAI(\n#     openai_api_version=\"2023-05-15\",\n#     azure_endpoint=azure_configs[\"base_url\"],\n#     azure_deployment=azure_configs[\"model_deployment\"],\n#     model=azure_configs[\"model_name\"],\n#     openai_api_key=\"\",\n#     validate_base_url=False,\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from ragas import EvaluationDataset, evaluate\n# from ragas.llms import LangchainLLMWrapper\n# from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n\n# # Initialize query, reference, and RAG model\n# query = question\n# reference = \"The Jedi Padawan accompanying Obi-Wan on this mission is Anakin Skywalker.\"\n\n\n# # Retrieve relevant documents and generate response\n# relevant_docs = reordered_responses\n# response = output[0][\"generated_text\"]\n\n# # Create the evaluation dataset for a single query\n# dataset = [\n#     {\n#         \"user_input\": query,\n#         \"retrieved_contexts\": relevant_docs,\n#         \"response\": response,\n#         \"reference\": reference\n#     }\n# ]\n# evaluation_dataset = EvaluationDataset.from_list(dataset)\n\n# # Initialize evaluator and evaluate\n# evaluator_llm = LangchainLLMWrapper(azure_llm)\n# metrics = [LLMContextRecall(), Faithfulness(), FactualCorrectness()]\n# result = evaluate(dataset=evaluation_dataset, metrics=metrics, llm=evaluator_llm)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Query: \" ,question)\n# print(\"\")\n# print(\"Response: \" ,textwrap.fill(response, width=80))\n# print(\"\")\n# print(\"Golden Answer: \",textwrap.fill(reference))\n# print(\"\")\n# print(\"Result:\",result)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}